{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution To The Problem Statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from scrape import scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.fallingrain.com\" \n",
    "page = urllib.request.urlopen(url+\"/world/IN/\")\n",
    "soup = BeautifulSoup(page, 'html.parser')           # Extracting the home page into a soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['State','City','Latitude','Longitude','Elevation','Estimated Population'] )    # Creating a DataFrame\n",
    "df.to_csv(\"Directory_of_Regions_In_India.csv\", mode='a', index=False) # For writing the headings to the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in soup.find(\"ul\").contents:  # Since the name of the states were listed under the <ul> tag, iterating through throught the states\n",
    "    df = pd.DataFrame(columns=['State','City','Latitude','Longitude','Elevation','Estimated Population']) #Initialising Dataframe for new state\n",
    "    if isinstance(state, Tag):\n",
    "        name = state.text\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(url+state.a[\"href\"]), 'html.parser') # Creating a soup object of the link\n",
    "        \n",
    "        soup_stack=[soup] # To stack the soup objects of each link      \n",
    "        \n",
    "        while True:\n",
    "            if(len(soup_stack)==0): # To test if extrated all links, by checking empty stack\n",
    "                break\n",
    "        \n",
    "            cur = soup_stack[-1].find(string=\"Alphabetical listing of Places in \"+name) # Bringing the current location to the heading\n",
    "           \n",
    "            if cur.find_next(string=\"Name\"):  # To check if it contains the required table \n",
    "                for var in cur.find_next(\"table\").contents: \n",
    "                    if isinstance(var, Tag):         \n",
    "                        if var.find(string=\"Name\"):   # To skip the first row of column names\n",
    "                            continue\n",
    "                        row = var.find_all(string=True)   # Making a list of the row \n",
    "                        if(name==\"None\"):\n",
    "                            row.insert(4, \"None\")    # Since None doesn't have a string under the column Region\n",
    "                        print(row)                   # Just to check the correct Values while execution\n",
    "\n",
    "                        # Appending the required values to the dataframe \n",
    "                        \n",
    "                        df=df.append({'State':row[4],'City':row[0],'Latitude':row[8],'Longitude':row[10],'Elevation':row[12],'Estimated Population':row[14]}, ignore_index=True)\n",
    "\n",
    "                soup_stack.pop() #popping the last object stored\n",
    "\n",
    "                        \n",
    "            else: # implies the page does not contain the table this is not the last layer\n",
    "                soup_stack.pop()  #pop the stack to remove the current link\n",
    "                \n",
    "                 \n",
    "                pool = mp.Pool()  # Creating pool object\n",
    "\n",
    "                urls = [url+var[\"href\"] for var in cur.find_all_next(\"a\")] #Creating a list of all the links on the page\n",
    "                results = pool.map(scrape, urls) # Parallelly extracting all the links\n",
    "                                                 # scrape function in a seperate file, since Jupyter Notebook doesn't allow multiprocessing on functions defined in the same file\n",
    "                        \n",
    "                pool.close()\n",
    "                pool.join()\n",
    "    \n",
    "                results=list(reversed(results)) # reversing the contents to ensure correct order while stacking\n",
    "                for page in results:\n",
    "                    soup_stack.append(BeautifulSoup(page, 'html.parser')) # stacking all the links\n",
    "\n",
    "    \n",
    "    df.to_csv(\"Directory_of_Regions_In_India.csv\", mode='a', header=False, index=False) # Writing to CSV for each state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
