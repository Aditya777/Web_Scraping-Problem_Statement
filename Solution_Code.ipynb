{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution To The Problem Statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.fallingrain.com\" \n",
    "page = urllib.request.urlopen(url+\"/world/IN/\")\n",
    "soup = BeautifulSoup(page, 'html.parser')           # Extracting the home page into a soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['State','City','Latitude','Longitude','Elevation','Estimated Population'] )    # Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in soup.find(\"ul\").contents:   # Since the name of the states were listed under the <ul> tag\n",
    "    if isinstance(state, Tag):\n",
    "        name = state.text\n",
    "        state_soup = BeautifulSoup(urllib.request.urlopen(url+state.a[\"href\"]), 'html.parser') # Creating a soup object of the link\n",
    "        df = extract_data(state_soup, name, df)   # Function Call\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Directory_of_Regions_In_India.csv\", index =False, encoding='utf-8') # Converting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE EXTRACT_DATA FUNCTION\n",
    "\n",
    "def extract_data(soup, name,df):\n",
    "    cur = soup.find(string=\"Alphabetical listing of Places in \"+name) # Bringing the current location to the heading\n",
    "    if cur.find_next(string=\"Name\"):  # To check if its the required table under it or not\n",
    "        for var in cur.find_next(\"table\").contents: \n",
    "            if isinstance(var, Tag):         \n",
    "                if var.find(string=\"Name\"):   # To skip the first row of column names\n",
    "                    continue\n",
    "                row = var.find_all(string=True)   # Making a list of the row \n",
    "                if(name==\"None\"):\n",
    "                    row.insert(4, \"None\")    # Since None doesn't have a string under the column Region\n",
    "                print(row)                   # Just to check the correct Values while execution\n",
    "                   \n",
    "                # Appending the required values to the dataframe \n",
    "                df = df.append({'State':row[4],'City':row[0],'Latitude':row[8],'Longitude':row[10],'Elevation':row[12],'Estimated Population':row[14]}, ignore_index=True)\n",
    "                                               \n",
    "    else:\n",
    "        for var in cur.find_all_next(\"a\"):\n",
    "            print(var)      # To check the link being extracted\n",
    "            soup = BeautifulSoup(urllib.request.urlopen(url+var[\"href\"]), 'html.parser') \n",
    "            df =  extract_data(soup, name, df)   # Extracting each link and recursing untill table is found\n",
    "  \n",
    "            \n",
    "    return df    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
